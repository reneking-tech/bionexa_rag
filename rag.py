# rag.py

import numpy as np
import pickle
import openai
from dotenv import load_dotenv
import os

# Only needed locally, not on Streamlit Cloud
load_dotenv()

# Load saved FAISS index, raw documents, and full record rows
with open("kb.pkl", "rb") as f:
    index, docs, records = pickle.load(f)

def retrieve_top_k(query, k=5):
    """
    Embed the query and return top-k matching records using FAISS.
    """
    q_embed = openai.embeddings.create(
        model="text-embedding-3-small",
        input=query
    ).data[0].embedding

    D, I = index.search(np.array([q_embed]).astype("float32"), k)
    return [records[i] for i in I[0]]  # Return full record dicts

def generate_answer(query):
    query_lower = query.lower()

    if "where" in query_lower and "drop" in query_lower:
        return (
            "üìç Samples can be dropped at:\n"
            "**Modderfontein Industrial Complex**, Standerton Avenue, via Nobel Gate, Modderfontein, Gauteng, South Africa, 1645.\n"
            "Please go to the **permit office on the corner of Nobel Avenue and Standerton Road**.\n"
            "Ensure all bottles are clearly labeled before delivery."
        )

    context_rows = retrieve_top_k(query, k=5)

    context = "\n".join(
        f"‚Ä¢ Test: {r.get('test_name', '')} | Price: R{r.get('price_ZAR', 'N/A')} | "
        f"Turnaround: {r.get('turnaround_days', 'N/A')} days | "
        f"Sample prep: {str(r.get('sample_prep') or '')} | "
        f"Notes: {str(r.get('notes') or '')}"
        for r in context_rows
    )

    system_prompt = (
        "You are a helpful assistant for Bionexa Lab. Use the context below to answer customer questions "
        "about test pricing, turnaround times, sample preparation, or drop-off procedures. "
        "If the context includes a physical address or instructions for sample delivery, include those clearly in your reply. "
        "Do your best to infer answers from the examples. Only say 'I‚Äôm not 100% sure ‚Äì let me arrange a call with our support team.' "
        "if there is truly no relevant information."
    )

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"Question: {query}\n\nContext:\n{context}"}
    ]

    response = openai.chat.completions.create(
        model="gpt-4o",
        messages=messages
    )

    return response.choices[0].message.content
